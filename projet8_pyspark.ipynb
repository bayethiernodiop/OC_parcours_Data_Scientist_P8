{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CONFIGURATION** de PySpark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# LOCAL RUN: \n",
    "# spark-submit --packages databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark_app.py .\n",
    "\n",
    "# CLUSTER RUN:\n",
    "# spark-submit --master yarn --packages databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 --deploy-mode cluster pyspark_app.py s3://p8-fruits/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration des variables d'environnement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set environment variables\n",
    "%env SPARK_LOCAL_HOSTNAME=localhost\n",
    "%env PYSPARK_DRIVER_PYTHON=ipython3\n",
    "%env PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" ./bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification des variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christophe/spark/spark-2.4.4-bin-hadoop2.7\n",
      "/usr/bin:/home/christophe/spark/spark-2.4.4-bin-hadoop2.7/bin:/home/christophe/anaconda3/envs/DS_projet8/bin:/home/christophe/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "/usr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['PATH'])\n",
    "print(os.environ['JAVA_HOME'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciation de l'objet *SparkSession*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SparkSession\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"Projet 8\")\n",
    "            .config('spark.jars.packages', 'databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11')\n",
    "            .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sparkdl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark.conf.set('spark.jars.packages', 'databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation d'un package Maven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mvn install:install-file -Dfile=/home/christophe/Insync/lebrun_christo@yahoo.fr/OneDrive/OpenClassrooms/parcoursDS/projet8/s3/libs/spark-deep-learning-1.5.0-spark2.4-s_2.11.jar -DgroupId=databricks \\\n",
    "        -DartifactId=spark-deep-learning -Dversion=1.5.0-spark2.4-s_2.11 -Dpackaging=jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essai d'installation de dépendance locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the sparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# load pyspark with databricks package\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages databricks:spark-deep-learning:1.5.0-spark2.4-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# load pyspark with databricks package\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/christophe/Desktop/Link\\ to\\ projet8/s3/libs/spark-deep-learning-1.5.0-spark2.4-s_2.11.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SparkSession\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"Projet 8\")\n",
    "            .config('spark.jars', \"/home/christophe/Desktop/Link\\ to\\ projet8/s3/libs/spark-deep-learning-1.5.0-spark2.4-s_2.11.jar\")\n",
    "            .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SparkSession\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"Projet 8\")\n",
    "            .config('spark.jars', \"/home/christophe/Insync/lebrun_christo@yahoo.fr/OneDrive/OpenClassrooms/parcoursDS/projet8/s3/libs/spark-deep-learning-1.5.0-spark2.4-s_2.11.jar\")\n",
    "            .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "conf = SparkConf().set(\"spark.jars\", \"/home/christophe/Desktop/Link\\ to\\ projet8/s3/libs/spark-deep-learning-1.5.0-spark2.4-s_2.11.jar\")\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sparkdl import DeepImageFeaturizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXTRACTION** des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression des espaces dans les noms de répertoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializations\n",
    "path = './data/'\n",
    "\n",
    "# Parsing the path\n",
    "for root, dirs, files in os.walk(path):\n",
    "    \n",
    "    # Checking if last level of directory\n",
    "    if dirs==[]:\n",
    "        \n",
    "        # Get the name of the directory\n",
    "        old_dir = root.split(\"/\")[-1]\n",
    "        \n",
    "        # Checking if the name contains blank spaces\n",
    "        if \" \" in old_dir:\n",
    "\n",
    "            # Get the name of the base\n",
    "            base = root.strip(old_dir)\n",
    "            # Create new name for directory\n",
    "            new_dir = old_dir.replace(\" \", \"_\")\n",
    "\n",
    "            print(\"Rename:\", old_dir, \"to:\", new_dir)\n",
    "            os.rename(os.path.join(base, old_dir), os.path.join(base, new_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des images et association des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pear_Kaiser\n",
      "Cocos\n",
      "Mango_Red\n",
      "Potato_Sweet\n",
      "Pomelo_Sweetie\n",
      "Limes\n",
      "Huckleberry\n",
      "Clementine\n",
      "Potato_White\n",
      "Nectarine\n",
      "Loading completed.\n"
     ]
    }
   ],
   "source": [
    "## Parsing directory\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Initializations\n",
    "path = './s3/data/Test/'\n",
    "i = 0\n",
    "\n",
    "# Parsing the directories and files in the path\n",
    "for root, dirs, files in os.walk(path):\n",
    "    \n",
    "    # If no sub-directories (limit to 10 categories for exploration)\n",
    "    if dirs==[] and i<10:\n",
    "        \n",
    "        # Get the name of the last directory\n",
    "        category = root.split(\"/\")[-1]\n",
    "        print(category)\n",
    "\n",
    "        # Read image data using new image scheme\n",
    "        cat_df = (spark\n",
    "                .read\n",
    "                .format(\"image\")\n",
    "                .load(root)\n",
    "        )\n",
    "        \n",
    "        # Displays a few rows\n",
    "        # cat_df.show(3)\n",
    "        \n",
    "        # Add a column with label\n",
    "        cat_df = cat_df.withColumn(\n",
    "            'label',\n",
    "            lit(category),\n",
    "        )\n",
    "        \n",
    "        # Contatenate the data\n",
    "        try:\n",
    "            image_df = image_df.unionAll(cat_df)\n",
    "        except NameError:\n",
    "            image_df = cat_df\n",
    "        \n",
    "        # Increment counter (for exploration)\n",
    "        i += 1\n",
    "\n",
    "print(\"Loading completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               image|      label|\n",
      "+--------------------+-----------+\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = image_df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               image|      label|\n",
      "+--------------------+-----------+\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "|[file:///home/chr...|Pear_Kaiser|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df = image_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Parsing directory\n",
    "import os\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Directory of the images\n",
    "path = './s3/data/Test/'\n",
    "path = os.path.normpath(path)\n",
    "        \n",
    "# Read image data using new image scheme\n",
    "cat_df = (spark\n",
    "        .read\n",
    "        .format(\"image\")\n",
    "        .load(path)\n",
    ") \n",
    "\n",
    "print(\"Loading completed.\")\n",
    "cat_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##################################\n",
    "    # Loading images\n",
    "    ##################################\n",
    "\n",
    "    ROOT_PATH = sys.argv[1]\n",
    "\n",
    "    # RUN MODE\n",
    "    DATA_PATH = os.path.join(ROOT_PATH, \"data/\")\n",
    "\n",
    "    # TEST MODE\n",
    "    DATA_PATH = os.path.join(ROOT_PATH, \"data/Test/Apricot/\") # REMOVE the line for production\n",
    "\n",
    "    image_df = spark.read.format(\"image\").load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, element_at\n",
    "\n",
    "# Getting the label by splitting the path of the image and getting its last directory\n",
    "image_df = (image_df\n",
    "            .withColumn(\n",
    "                'label',\n",
    "                element_at(\n",
    "                    split(\n",
    "                        image_df['image.origin'],\n",
    "                        \"/\"),\n",
    "                    -2))\n",
    "            .show(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = image_df.count()\n",
    "print(\"Total number of images:\", img_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of categories:\")\n",
    "image_df.select('label').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST FOR DEBUG…\n",
    "\n",
    "# sample_img_dir = './data/Training/Banana'\n",
    "sample_img_dir = 'D:/Banana'\n",
    "\n",
    "# Read image data using new image scheme\n",
    "image_df = (spark\n",
    "        .read\n",
    "        .option(\"badRecordsPath\", \"/tmp/badRecordsPath\")\n",
    "        .format(\"image\")\n",
    "        .load(sample_img_dir)\n",
    ")\n",
    "\n",
    "image_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceding to data of images\n",
    "nb_height = image_df.select(\"image.height\").distinct().count()\n",
    "print(\"Nombre de hauteurs différentes:\", nb_height)\n",
    "\n",
    "nb_width = image_df.select(\"image.width\").distinct().count()\n",
    "print(\"Nombre de largeurs différentes:\", nb_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TRAITEMENT** des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importations des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkdl import DeepImageFeaturizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction de caractéristiques par CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = DeepImageFeaturizer(\n",
    "    inputCol=\"image\",\n",
    "    outputCol=\"features\",\n",
    "    modelName=\"ResNet50\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sparkdl --upgrade\n",
    "!pip show sparkdl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Using SparkDL instead of new Spark read image. Required for DeepImageFeaturizer ?\n",
    "\n",
    "from sparkdl import readImages\n",
    "image_df = readImages('D:/Banana')\n",
    "\n",
    "image_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our model\n",
    "features_df = featurizer.transform(image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for debug purpose…\n",
    "features_df = image_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration de Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import config.p8_fruits_credentials as credentials\n",
    "\n",
    "# Get the flickr_key and flickr_secret\n",
    "ACCESS_KEY = credentials.ACCESS_KEY\n",
    "SECRET_KEY = credentials.SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    region_name='eu-west-3',\n",
    ")\n",
    "\n",
    "my_bucket = s3_resource.Bucket('p8-fruits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléversement sur S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a file on the S3 bucket in the 'results' directory\n",
    "my_bucket.put_object(\n",
    "    Body=str(img_count), # content of the file to save\n",
    "    Key='results/hello.txt', # path of the new_file on the bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file back and read it\n",
    "my_bucket.download_file(Key='results/hello.txt', Filename='hello_back.txt')\n",
    "open('hello_back.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file back and read it\n",
    "my_bucket.download_file(Key='results/features.txt', Filename='features.txt')\n",
    "open('features.txt').read()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################\n",
    "# Log results to S3, using boto3\n",
    "####################################\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Create a resource object\n",
    "s3_resource = boto3.resource(\n",
    "    's3',\n",
    "    region_name='eu-west-3',\n",
    ")\n",
    "\n",
    "# Get the S3 bucket\n",
    "my_bucket = s3_resource.Bucket('p8-fruits')\n",
    "\n",
    "# Save a file on the S3 bucket in the 'results' directory\n",
    "my_bucket.put_object(\n",
    "    Body=str(img_count), # content of the file to save\n",
    "    Key='results/features.txt', # path of the new_file on the bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXTINCTION** de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the sparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LECTURE** des résultas (format *parquet*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = spark.read.format(\"parquet\").load(\"s3/results/Dec-20-2019 09:25:49\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|            features|         label|              origin|\n",
      "+--------------------+--------------+--------------------+\n",
      "|[2.20138478279113...|     Raspberry|file:/home/christ...|\n",
      "|[2.03587174415588...|     Raspberry|file:/home/christ...|\n",
      "|[0.29767277836799...|Pineapple_Mini|file:/home/christ...|\n",
      "|[2.59762907028198...|     Raspberry|file:/home/christ...|\n",
      "|[2.10859084129333...|     Raspberry|file:/home/christ...|\n",
      "+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: string, origin: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.limit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+\n",
      "|            features|         label|              origin|\n",
      "+--------------------+--------------+--------------------+\n",
      "|[2.20138478279113...|     Raspberry|file:/home/christ...|\n",
      "|[2.03587174415588...|     Raspberry|file:/home/christ...|\n",
      "|[0.29767277836799...|Pineapple_Mini|file:/home/christ...|\n",
      "|[2.59762907028198...|     Raspberry|file:/home/christ...|\n",
      "|[2.10859084129333...|     Raspberry|file:/home/christ...|\n",
      "+--------------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **QUESTIONS / PROBLÈMES TECHNIQUES**\n",
    "\n",
    "\n",
    "* Plusieurs options pour la gestion de la mémoire :\n",
    "    * executor-cores\n",
    "    * nombre de partitions (dans le code du DF/RDD)\n",
    "    * -executor-memory 10g\n",
    "    * --driver-memory 10g\n",
    "    \n",
    "    \n",
    "* Options pour les notebooks\n",
    "    * Notebook EMR\n",
    "        * pb : configurer une SparkSession déjà initiée\n",
    "        * pb : donner l'accès à Internet (vérifier avec Ping)\n",
    "        * copier le .jar sur le master\n",
    "    * Jupyter sur le master node\n",
    "    * Zeppelin\n",
    "    * DataBricks\n",
    "    * Google collabs\n",
    "    * SageMaker notebooks\n",
    "    * …\n",
    " \n",
    " \n",
    "* Logiciels sur clusters\n",
    "    * Spark\n",
    "    * Livy (pour notebooks)\n",
    "    * Hadoop\n",
    "    * Ganglia\n",
    "    * Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Rendre public les fichiers de données et les résultats sur S3 ? Le notebook ?\n",
    "* Voir le besoin d'installer les modules (sys, os) sur les nodes ou le master avant l'exécution\n",
    "* Importer .jar avec notebook EMR… Inclure le paquet local comme dépendence Maven ?\n",
    "    * https://docs.amazonaws.cn/en_us/emr/latest/ManagementGuide/emr-managed-notebooks-considerations.html\n",
    "    * https://stackoverflow.com/questions/57473914/adding-external-jars-in-emr-notebooks\n",
    "* Prétraitements : \n",
    "    * https://databricks.com/blog/2018/12/10/introducing-built-in-image-data-source-in-apache-spark-2-4.html\n",
    "* 1'000 images, est-ce suffisant ? 1 heure de travail, sur 1 PC… Ou essai sur 2 PC ?\n",
    "* Split des labels : \"Apple Golden 1\"\n",
    "* Différence entre mode cluster et mode client ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
